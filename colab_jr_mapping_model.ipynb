{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Main Model\n",
    "This file contains all the code written for my Bachelor Thesis regarding the main model.\n",
    "\n",
    "This model is able to classify scientific papers into 1 of the 17 different SDGs.\n",
    "\n",
    "The workflow contains:\n",
    "- Reading the data\n",
    "- Cleaning the data\n",
    "- Preparing the data\n",
    "- Quality checking the data\n",
    "- Plots\n",
    "- Main modeling part\n",
    "- Evaluation part\n",
    "\n",
    "To check the complete worflow please refer to the other file in this folder: \"SDG_mapping_complete_workflow.ipynb\"\n",
    "\n",
    "Regular quality checks are done and will not be described explicitly."
   ],
   "metadata": {
    "id": "2vxsr9BMU8Kg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation and Imports"
   ],
   "metadata": {
    "id": "J83p8kCylUsw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langdetect"
   ],
   "metadata": {
    "id": "lrMc2EYamXDV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pandas"
   ],
   "metadata": {
    "id": "AkLz37Gy39U1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m281.4/281.4 MB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.7/199.7 KB\u001B[0m \u001B[31m23.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=1e046ba7c4318e7b86936b58d0eec6c86e925c3b811b2f7078cde0845f3eeb8e\n",
      "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ],
   "metadata": {
    "id": "tnGCwkFA39U2",
    "outputId": "c002d1de-811d-4c64-f3af-a402b5348373",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import array\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "id": "bq39Q92Vim_e"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the data"
   ],
   "metadata": {
    "id": "tx8GXls9lriA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('osdg-community-data_high_IAA.tsv', sep='\\t')\n",
    "df.info()"
   ],
   "metadata": {
    "id": "a-SWiRtmim0d",
    "outputId": "9db02eb1-30c6-43f3-d959-35d257fa3e1b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28649 entries, 0 to 28648\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   doi              28649 non-null  object \n",
      " 1   text_id          28649 non-null  object \n",
      " 2   text             28649 non-null  object \n",
      " 3   sdg              28649 non-null  int64  \n",
      " 4   labels_negative  28649 non-null  int64  \n",
      " 5   labels_positive  28649 non-null  int64  \n",
      " 6   agreement        28649 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# df.shape\n",
    "df.groupby('sdg').count()\n",
    "# display(df)\n",
    "# df.dtypes"
   ],
   "metadata": {
    "id": "W75Rsz0Vim5A",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "outputId": "47f96cbe-cf32-479c-f721-eed0ade4190e"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      doi  text_id  text  labels_negative  labels_positive  agreement\n",
       "sdg                                                                  \n",
       "1    1743     1743  1743             1743             1743       1743\n",
       "2    1386     1386  1386             1386             1386       1386\n",
       "3    2187     2187  2187             2187             2187       2187\n",
       "4    2983     2983  2983             2983             2983       2983\n",
       "5    3401     3401  3401             3401             3401       3401\n",
       "6    1879     1879  1879             1879             1879       1879\n",
       "7    2398     2398  2398             2398             2398       2398\n",
       "8    1104     1104  1104             1104             1104       1104\n",
       "9    1066     1066  1066             1066             1066       1066\n",
       "10    945      945   945              945              945        945\n",
       "11   1616     1616  1616             1616             1616       1616\n",
       "12    718      718   718              718              718        718\n",
       "13   1535     1535  1535             1535             1535       1535\n",
       "14    960      960   960              960              960        960\n",
       "15   1113     1113  1113             1113             1113       1113\n",
       "16   3615     3615  3615             3615             3615       3615"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-6431e6aa-2265-4bc8-a0e2-1f2d67e57fcd\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels_negative</th>\n",
       "      <th>labels_positive</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sdg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743</td>\n",
       "      <td>1743</td>\n",
       "      <td>1743</td>\n",
       "      <td>1743</td>\n",
       "      <td>1743</td>\n",
       "      <td>1743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1386</td>\n",
       "      <td>1386</td>\n",
       "      <td>1386</td>\n",
       "      <td>1386</td>\n",
       "      <td>1386</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2187</td>\n",
       "      <td>2187</td>\n",
       "      <td>2187</td>\n",
       "      <td>2187</td>\n",
       "      <td>2187</td>\n",
       "      <td>2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2983</td>\n",
       "      <td>2983</td>\n",
       "      <td>2983</td>\n",
       "      <td>2983</td>\n",
       "      <td>2983</td>\n",
       "      <td>2983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1879</td>\n",
       "      <td>1879</td>\n",
       "      <td>1879</td>\n",
       "      <td>1879</td>\n",
       "      <td>1879</td>\n",
       "      <td>1879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2398</td>\n",
       "      <td>2398</td>\n",
       "      <td>2398</td>\n",
       "      <td>2398</td>\n",
       "      <td>2398</td>\n",
       "      <td>2398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1066</td>\n",
       "      <td>1066</td>\n",
       "      <td>1066</td>\n",
       "      <td>1066</td>\n",
       "      <td>1066</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1616</td>\n",
       "      <td>1616</td>\n",
       "      <td>1616</td>\n",
       "      <td>1616</td>\n",
       "      <td>1616</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1535</td>\n",
       "      <td>1535</td>\n",
       "      <td>1535</td>\n",
       "      <td>1535</td>\n",
       "      <td>1535</td>\n",
       "      <td>1535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3615</td>\n",
       "      <td>3615</td>\n",
       "      <td>3615</td>\n",
       "      <td>3615</td>\n",
       "      <td>3615</td>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6431e6aa-2265-4bc8-a0e2-1f2d67e57fcd')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6431e6aa-2265-4bc8-a0e2-1f2d67e57fcd button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6431e6aa-2265-4bc8-a0e2-1f2d67e57fcd');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleaning and processing\n",
    "In this code section the data is being cleaned and processed."
   ],
   "metadata": {
    "id": "QbSGQ0_gme1X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define coltypes\n",
    "\n",
    "df = df.astype({'doi':'string'})\n",
    "df = df.astype({'text_id':'string'})\n",
    "df = df.astype({'text':'string'})\n",
    "df = df.astype({'sdg':'string'})\n",
    "df = df.astype({'labels_negative':'int'})\n",
    "df = df.astype({'labels_positive':'int'})\n",
    "df = df.astype({'agreement':'int'})\n",
    "\n",
    "# df[['kurzfassung']] = df[['kurzfassung']].fillna(value='unknown')\n",
    "# df = df.drop(df[df.kurzfassung == 'unknown'].index)\n",
    "\n",
    "# def language_detect(x):\n",
    "#     lang = detect(x)\n",
    "#     return lang\n",
    "#\n",
    "# df['language'] = df['kurzfassung'].apply(language_detect)\n",
    "# df.groupby('language').count()\n",
    "# df = df.astype({'language':'string'})\n",
    "# df = df.drop(df[df.language == 'de'].index)\n",
    "# df.groupby('language').count()"
   ],
   "metadata": {
    "id": "VFvl0BGOim82"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SDGs to int\n",
    "\n",
    "df['sdg'] = df['sdg'].str.replace('SDG', '') # remove SDG\n",
    "df['sdg'] = df['sdg'].astype('int') # change column type to integer"
   ],
   "metadata": {
    "id": "OAVCY5dThxXG"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "id": "m9mAqHogX0-H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1bc863d1-bb2b-416b-b9a8-08a95b7fafd1"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['doi', 'text_id', 'text', 'sdg', 'labels_negative', 'labels_positive',\n",
       "       'agreement'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark\n",
    "\n",
    "In this section PySpark is being used for:\n",
    "- Word2Vec\n",
    "- StringIndexing\n",
    "- OneHotEncoding\n"
   ],
   "metadata": {
    "id": "x3Qr04z9nUv3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ],
   "metadata": {
    "id": "SfUFsT51qUZG"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df_arrayed = spark_df.withColumn(\"doi\", array(spark_df[\"doi\"]))\n",
    "spark_df_arrayed = spark_df.withColumn(\"text_id\", array(spark_df[\"text_id\"]))\n",
    "spark_df_arrayed = spark_df.withColumn(\"text\", array(spark_df[\"text\"]))"
   ],
   "metadata": {
    "id": "i7z2hlbrqn7e"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df_arrayed.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syetJUMdrBci",
    "outputId": "d74c98d4-7fd3-487a-94d5-3b0f16095b86"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- doi: string (nullable = true)\n",
      " |-- text_id: string (nullable = true)\n",
      " |-- text: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- labels_negative: long (nullable = true)\n",
      " |-- labels_positive: long (nullable = true)\n",
      " |-- agreement: long (nullable = true)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Word vectors for text columns\n",
    "\n",
    "def word_vectorizer(df, col, col_new):\n",
    "    #create an average word vector for text columns\n",
    "    word2vec = Word2Vec(vectorSize = 10, minCount = 1, inputCol = col, outputCol = col_new)\n",
    "    model = word2vec.fit(df)\n",
    "    message_w2vec = model.transform(df)\n",
    "\n",
    "    # drop original column\n",
    "    message_w2vec = message_w2vec.drop(col)\n",
    "\n",
    "    return message_w2vec\n",
    "\n",
    "# TODO: Try creating a new df for each function call?\n",
    "# Apply function for text columns\n",
    "col = 'doi'\n",
    "col_new = 'doi_vectorized'\n",
    "spark_df_vectorized = word_vectorizer(spark_df_arrayed, col, col_new)\n",
    "\n",
    "col = 'text_id'\n",
    "col_new = 'text_id_vectorized'\n",
    "spark_df_vectorized = word_vectorizer(spark_df_vectorized, col, col_new)\n",
    "\n",
    "col = 'text'\n",
    "col_new = 'text_vectorized'\n",
    "spark_df_vectorized = word_vectorizer(spark_df_vectorized, col, col_new)"
   ],
   "metadata": {
    "id": "gSg4IbFDqm8S",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "outputId": "5616e93a-bd0f-4536-9d93-f1b2a28a5c5a"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "error",
     "ename": "IllegalArgumentException",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-40-d2a0c9b15b92>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'doi'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mcol_new\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'doi_vectorized'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0mspark_df_vectorized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword_vectorizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark_df_arrayed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_new\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'text_id'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-40-d2a0c9b15b92>\u001B[0m in \u001B[0;36mword_vectorizer\u001B[0;34m(df, col, col_new)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;31m#create an average word vector for text columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mword2vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWord2Vec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvectorSize\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mminCount\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputCol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcol_new\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword2vec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mmessage_w2vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 383\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    384\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    379\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 380\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column doi must be of type equal to one of the following types: [array<string>, array<string>] but was actually of type string."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df_vectorized.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJm_ws6Nr3wq",
    "outputId": "d82b62a2-05fc-40cd-ecf2-6150261d37c4"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- doi: string (nullable = true)\n",
      " |-- text_id: string (nullable = true)\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- labels_negative: long (nullable = true)\n",
      " |-- labels_positive: long (nullable = true)\n",
      " |-- agreement: long (nullable = true)\n",
      " |-- text_vectorized: vector (nullable = true)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Model\n",
    "In this section the main model is trained."
   ],
   "metadata": {
    "id": "ueidnpycZ3QL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# data split into train and test\n",
    "\n",
    "(trainDF, testDF) = spark_df_vectorized.randomSplit([0.7, 0.3], seed=12)\n",
    "\n",
    "trainDF2 = trainDF.groupby('sdg').count()\n",
    "trainDF2.show()\n",
    "\n",
    "testDF2 = testDF.groupby('sdg').count()\n",
    "testDF2.show()"
   ],
   "metadata": {
    "id": "49imssGvzKTv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# main model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MulticlassLogisticRegressionWithElasticNet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_df_renamed = spark_df_vectorized.withColumnRenamed('sdg', 'label')\n",
    "\n",
    "feature = spark_df_renamed.drop(\"label\").columns\n",
    "print(feature)\n",
    "\n",
    "(trainDF, testDF) = spark_df_renamed.randomSplit([0.7, 0.3], seed=12)\n",
    "\n",
    "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "vecAssembler = VectorAssembler(inputCols=feature, outputCol=\"features\")\n",
    "\n",
    "# maxIter=10, regParam=0.3, elasticNetParam=0.8\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#Build the Pipeline\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "\n",
    "# Create model.\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "    \n",
    "print(\"#######\\n#######\\n#######\\n\")\n",
    "print(\"Now printing: Model test\")\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i+1, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "    % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n",
    "\n",
    "print(\"#######\\n#######\\n#######\\n\")\n",
    "print(\"Now printing: Model validation prediction\")\n",
    "\n",
    "# Apply the pipeline model to the test dataset.\n",
    "validation_prediction = pipelineModel.transform(testDF)\n",
    "\n",
    "#Display the predictions from the model. The features column is a sparse vector, which is often the case after one-hot encoding, #because there are so many 0 values\n",
    "validation_prediction.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "#Evaluate the model\n",
    "#The display command has a built-in ROC curve option.\n",
    "\n",
    "print(pipelineModel.stages[-1], validation_prediction.drop(\"prediction\", \"rawPrediction\"), \"ROC\")\n",
    "\n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(validation_prediction)}\")\n"
   ],
   "metadata": {
    "id": "8LOx7O9-t_nW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Quality check on how the model performs on each different SDG\n",
    "from pyspark.sql.functions import sum, cols_to_drop_later, desc\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "validation_count = validation_prediction.groupby('label', 'prediction').count()\n",
    "validation_count = validation_count.sort(desc(\"count\"))\n",
    "validation_count.show(200)\n",
    "y = validation_prediction.count()\n",
    "\n",
    "\n",
    "data_collect = validation_count.collect()\n",
    "\n",
    "x = 0\n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_collect:\n",
    "    if row['label'] == row['prediction']:\n",
    "      x += row['count']\n",
    "\n",
    "acc = x / y\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "to_check_sdg = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "d_error_by_label = {}\n",
    "cols_to_drop_later = ['label', 'prediction']\n",
    "\n",
    "for iii in to_check_sdg:\n",
    "  new_df_validation = validation_count.filter(cols_to_drop_later(\"label\") == iii)\n",
    "  dfxx = new_df_validation.toPandas()\n",
    "  listxx = dfxx['count'].tolist()\n",
    "  #print(listxx)\n",
    "  count_total = 0.0\n",
    "  for x in listxx:\n",
    "    count_total += x\n",
    "  count_total *= 1.0\n",
    "  print(\"count_total\",count_total)\n",
    "\n",
    "  dfxxx = dfxx[dfxx['label'] != dfxx['prediction']]\n",
    "  listxx = dfxxx['count'].tolist()\n",
    "  count_total_errors = 0.0\n",
    "  for x in listxx:\n",
    "    count_total_errors += x\n",
    "  count_total_errors *= 1.0\n",
    "  print(\"count_total_errors\",count_total_errors)\n",
    "  d_error_by_label[iii] = 1 - count_total_errors/count_total\n",
    "\n",
    "print(\"errors:\", d_error_by_label)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuFBo_o0VVg7",
    "outputId": "fe24dd55-ba32-4795-a497-28825158c7ab"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|   15|      15.0|   13|\n",
      "|    3|       3.0|    7|\n",
      "|   13|      13.0|    7|\n",
      "|   16|      16.0|    6|\n",
      "|    5|       5.0|    4|\n",
      "|    8|      10.0|    4|\n",
      "|    9|      12.0|    3|\n",
      "|   12|      12.0|    3|\n",
      "|    2|       2.0|    3|\n",
      "|    7|       7.0|    3|\n",
      "|   15|      13.0|    3|\n",
      "|   15|      14.0|    3|\n",
      "|   12|      17.0|    2|\n",
      "|   10|      17.0|    2|\n",
      "|   16|       3.0|    2|\n",
      "|   15|       1.0|    2|\n",
      "|    9|       9.0|    2|\n",
      "|    8|       3.0|    2|\n",
      "|    8|       9.0|    2|\n",
      "|    5|       8.0|    1|\n",
      "|    3|      14.0|    1|\n",
      "|    3|      16.0|    1|\n",
      "|    5|      12.0|    1|\n",
      "|    6|      13.0|    1|\n",
      "|   10|      12.0|    1|\n",
      "|   10|      16.0|    1|\n",
      "|   10|       5.0|    1|\n",
      "|   11|      14.0|    1|\n",
      "|    8|       8.0|    1|\n",
      "|   10|      10.0|    1|\n",
      "|   13|      15.0|    1|\n",
      "|    3|      10.0|    1|\n",
      "|    1|       8.0|    1|\n",
      "|    4|       8.0|    1|\n",
      "|    5|      10.0|    1|\n",
      "|    7|       2.0|    1|\n",
      "|    1|       1.0|    1|\n",
      "|   14|      14.0|    1|\n",
      "|   15|       8.0|    1|\n",
      "|   16|      10.0|    1|\n",
      "|   14|       8.0|    1|\n",
      "|   17|      15.0|    1|\n",
      "|   13|      17.0|    1|\n",
      "|   12|      10.0|    1|\n",
      "|   10|       8.0|    1|\n",
      "|    6|      10.0|    1|\n",
      "|   10|       3.0|    1|\n",
      "|    3|       5.0|    1|\n",
      "|   17|      10.0|    1|\n",
      "|   17|      12.0|    1|\n",
      "|   13|      12.0|    1|\n",
      "|   14|      15.0|    1|\n",
      "|   13|      10.0|    1|\n",
      "|   12|       6.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n",
      "accuracy: 0.48148148148148145\n",
      "count_total 2.0\n",
      "count_total_errors 1.0\n",
      "count_total 3.0\n",
      "count_total_errors 0.0\n",
      "count_total 11.0\n",
      "count_total_errors 4.0\n",
      "count_total 1.0\n",
      "count_total_errors 1.0\n",
      "count_total 7.0\n",
      "count_total_errors 3.0\n",
      "count_total 2.0\n",
      "count_total_errors 2.0\n",
      "count_total 4.0\n",
      "count_total_errors 1.0\n",
      "count_total 9.0\n",
      "count_total_errors 8.0\n",
      "count_total 5.0\n",
      "count_total_errors 3.0\n",
      "count_total 8.0\n",
      "count_total_errors 7.0\n",
      "count_total 1.0\n",
      "count_total_errors 1.0\n",
      "count_total 7.0\n",
      "count_total_errors 4.0\n",
      "count_total 11.0\n",
      "count_total_errors 4.0\n",
      "count_total 3.0\n",
      "count_total_errors 2.0\n",
      "count_total 22.0\n",
      "count_total_errors 9.0\n",
      "count_total 9.0\n",
      "count_total_errors 3.0\n",
      "count_total 3.0\n",
      "count_total_errors 3.0\n",
      "errors: {1: 0.5, 2: 1.0, 3: 0.6363636363636364, 4: 0.0, 5: 0.5714285714285714, 6: 0.0, 7: 0.75, 8: 0.11111111111111116, 9: 0.4, 10: 0.125, 11: 0.0, 12: 0.4285714285714286, 13: 0.6363636363636364, 14: 0.33333333333333337, 15: 0.5909090909090908, 16: 0.6666666666666667, 17: 0.0}\n"
     ]
    }
   ]
  }
 ]
}
