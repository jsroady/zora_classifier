{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Main Model\n",
    "This file contains all the code written for my Bachelor Thesis regarding the main model.\n",
    "\n",
    "This model is able to classify scientific papers into 1 of the 17 different SDGs.\n",
    "\n",
    "The workflow contains:\n",
    "- Reading the data\n",
    "- Cleaning the data\n",
    "- Preparing the data\n",
    "- Quality checking the data\n",
    "- Plots\n",
    "- Main modeling part\n",
    "- Evaluation part\n",
    "\n",
    "To check the complete worflow please refer to the other file in this folder: \"SDG_mapping_complete_workflow.ipynb\"\n",
    "\n",
    "Regular quality checks are done and will not be described explicitly."
   ],
   "metadata": {
    "id": "2vxsr9BMU8Kg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation and Imports"
   ],
   "metadata": {
    "id": "J83p8kCylUsw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langdetect"
   ],
   "metadata": {
    "id": "lrMc2EYamXDV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7096aa82-93a6-4614-e57b-da657e92f486"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\r\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\r\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langdetect) (1.16.0)\r\n",
      "Installing collected packages: langdetect\r\n",
      "Successfully installed langdetect-1.0.9\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1058fc250>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/pandas/\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107b7db50>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/pandas/\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107b7e190>: Failed to establish a new connection: [Errno 51] Network is unreachable')': /simple/pandas/\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107b7eb10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/pandas/\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107b7f550>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/pandas/\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting pandas\r\n",
      "  Using cached pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Collecting pytz>=2020.1\r\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m502.3/502.3 kB\u001B[0m \u001B[31m656.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "Installing collected packages: pytz, pandas\r\n",
      "Successfully installed pandas-1.5.3 pytz-2023.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\r\n",
      "\u001B[2K     \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.7/281.4 MB\u001B[0m \u001B[31m478.8 kB/s\u001B[0m eta \u001B[36m0:08:25\u001B[0m\r\n",
      "\u001B[?25h\u001B[31mERROR: Exception:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\r\n",
      "    yield\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\r\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\r\n",
      "           ^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\r\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\r\n",
      "           ^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\r\n",
      "    data = self.__fp.read(amt)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 465, in read\r\n",
      "    s = self.fp.read(amt)\r\n",
      "        ^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 705, in readinto\r\n",
      "    return self._sock.recv_into(b)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1278, in recv_into\r\n",
      "    return self.read(nbytes, buffer)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1134, in read\r\n",
      "    return self._sslobj.read(len, buffer)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "TimeoutError: The read operation timed out\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\r\n",
      "    status = run_func(*args)\r\n",
      "             ^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\r\n",
      "    return func(self, options, args)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 419, in run\r\n",
      "    requirement_set = resolver.resolve(\r\n",
      "                      ^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\r\n",
      "    result = self._result = resolver.resolve(\r\n",
      "                            ^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\r\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\r\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 348, in resolve\r\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\r\n",
      "    if not criterion.candidates:\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\r\n",
      "    return bool(self._sequence)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\r\n",
      "    return any(self)\r\n",
      "           ^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\r\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\r\n",
      "    candidate = func()\r\n",
      "                ^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\r\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\r\n",
      "                                       ^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\r\n",
      "    super().__init__(\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\r\n",
      "    self.dist = self._prepare()\r\n",
      "                ^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\r\n",
      "    dist = self._prepare_distribution()\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 308, in _prepare_distribution\r\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 491, in prepare_linked_requirement\r\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 536, in _prepare_linked_requirement\r\n",
      "    local_file = unpack_url(\r\n",
      "                 ^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\r\n",
      "    file = get_http_url(\r\n",
      "           ^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\r\n",
      "    from_path, content_type = download(link, temp_dir.path)\r\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 147, in __call__\r\n",
      "    for chunk in chunks:\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\r\n",
      "    for chunk in iterable:\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\r\n",
      "    for chunk in response.raw.stream(\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\r\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\r\n",
      "    with self._error_catcher():\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 155, in __exit__\r\n",
      "    self.gen.throw(typ, value, traceback)\r\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\r\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\r\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq39Q92Vim_e",
    "outputId": "90c8f771-da8a-4470-b424-361b8f8274d7"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the data"
   ],
   "metadata": {
    "id": "tx8GXls9lriA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/osdg-community-data_high_IAA.tsv', sep='\\t')\n",
    "df.info()"
   ],
   "metadata": {
    "id": "a-SWiRtmim0d"
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28649 entries, 0 to 28648\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   doi              28649 non-null  object \n",
      " 1   text_id          28649 non-null  object \n",
      " 2   text             28649 non-null  object \n",
      " 3   sdg              28649 non-null  int64  \n",
      " 4   labels_negative  28649 non-null  int64  \n",
      " 5   labels_positive  28649 non-null  int64  \n",
      " 6   agreement        28649 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# df.shape\n",
    "df.groupby('sdg').count()\n",
    "# display(df)\n",
    "# df.dtypes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "W75Rsz0Vim5A",
    "outputId": "1a215273-23a3-4fbb-c36b-ca3f0920d57e"
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      doi  text_id  text  labels_negative  labels_positive  agreement\nsdg                                                                  \n1    1743     1743  1743             1743             1743       1743\n2    1386     1386  1386             1386             1386       1386\n3    2187     2187  2187             2187             2187       2187\n4    2983     2983  2983             2983             2983       2983\n5    3401     3401  3401             3401             3401       3401\n6    1879     1879  1879             1879             1879       1879\n7    2398     2398  2398             2398             2398       2398\n8    1104     1104  1104             1104             1104       1104\n9    1066     1066  1066             1066             1066       1066\n10    945      945   945              945              945        945\n11   1616     1616  1616             1616             1616       1616\n12    718      718   718              718              718        718\n13   1535     1535  1535             1535             1535       1535\n14    960      960   960              960              960        960\n15   1113     1113  1113             1113             1113       1113\n16   3615     3615  3615             3615             3615       3615",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doi</th>\n      <th>text_id</th>\n      <th>text</th>\n      <th>labels_negative</th>\n      <th>labels_positive</th>\n      <th>agreement</th>\n    </tr>\n    <tr>\n      <th>sdg</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1743</td>\n      <td>1743</td>\n      <td>1743</td>\n      <td>1743</td>\n      <td>1743</td>\n      <td>1743</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1386</td>\n      <td>1386</td>\n      <td>1386</td>\n      <td>1386</td>\n      <td>1386</td>\n      <td>1386</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2187</td>\n      <td>2187</td>\n      <td>2187</td>\n      <td>2187</td>\n      <td>2187</td>\n      <td>2187</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2983</td>\n      <td>2983</td>\n      <td>2983</td>\n      <td>2983</td>\n      <td>2983</td>\n      <td>2983</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3401</td>\n      <td>3401</td>\n      <td>3401</td>\n      <td>3401</td>\n      <td>3401</td>\n      <td>3401</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1879</td>\n      <td>1879</td>\n      <td>1879</td>\n      <td>1879</td>\n      <td>1879</td>\n      <td>1879</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2398</td>\n      <td>2398</td>\n      <td>2398</td>\n      <td>2398</td>\n      <td>2398</td>\n      <td>2398</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1104</td>\n      <td>1104</td>\n      <td>1104</td>\n      <td>1104</td>\n      <td>1104</td>\n      <td>1104</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1066</td>\n      <td>1066</td>\n      <td>1066</td>\n      <td>1066</td>\n      <td>1066</td>\n      <td>1066</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>945</td>\n      <td>945</td>\n      <td>945</td>\n      <td>945</td>\n      <td>945</td>\n      <td>945</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1616</td>\n      <td>1616</td>\n      <td>1616</td>\n      <td>1616</td>\n      <td>1616</td>\n      <td>1616</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>718</td>\n      <td>718</td>\n      <td>718</td>\n      <td>718</td>\n      <td>718</td>\n      <td>718</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1535</td>\n      <td>1535</td>\n      <td>1535</td>\n      <td>1535</td>\n      <td>1535</td>\n      <td>1535</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>960</td>\n      <td>960</td>\n      <td>960</td>\n      <td>960</td>\n      <td>960</td>\n      <td>960</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1113</td>\n      <td>1113</td>\n      <td>1113</td>\n      <td>1113</td>\n      <td>1113</td>\n      <td>1113</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3615</td>\n      <td>3615</td>\n      <td>3615</td>\n      <td>3615</td>\n      <td>3615</td>\n      <td>3615</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleaning and processing\n",
    "In this code section the data is being cleaned and processed."
   ],
   "metadata": {
    "id": "QbSGQ0_gme1X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define coltypes\n",
    "\n",
    "df = df.astype({'doi':'string'})\n",
    "df = df.astype({'text_id':'string'})\n",
    "df = df.astype({'text':'string'})\n",
    "df = df.astype({'sdg':'string'})\n",
    "df = df.astype({'labels_negative':'int'})\n",
    "df = df.astype({'labels_positive':'int'})\n",
    "df = df.astype({'agreement':'int'})\n",
    "\n",
    "# df[['kurzfassung']] = df[['kurzfassung']].fillna(value='unknown')\n",
    "# df = df.drop(df[df.kurzfassung == 'unknown'].index)\n",
    "\n",
    "# def language_detect(x):\n",
    "#     lang = detect(x)\n",
    "#     return lang\n",
    "#\n",
    "# df['language'] = df['kurzfassung'].apply(language_detect)\n",
    "# df.groupby('language').count()\n",
    "# df = df.astype({'language':'string'})\n",
    "# df = df.drop(df[df.language == 'de'].index)\n",
    "# df.groupby('language').count()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "VFvl0BGOim82",
    "outputId": "e87772f0-c4c3-49e3-87ea-39471e10bf52"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # sdgs to numbers\n",
    "#\n",
    "# df['sdg'] = df['sdg'].str.replace('SDG','') # remove SDG\n",
    "# df['sdg'] = df['sdg'].astype('int') # change column type to integer"
   ],
   "metadata": {
    "id": "OAVCY5dThxXG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9mAqHogX0-H",
    "outputId": "35963a7b-c04a-4d85-9f39-41e590ced7bc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['sdg', 'author', 'kurzfassung', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark\n",
    "\n",
    "In this section PySpark is being used for:\n",
    "- Word2Vec\n",
    "- StringIndexing\n",
    "- OneHotEncoding\n"
   ],
   "metadata": {
    "id": "x3Qr04z9nUv3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ],
   "metadata": {
    "id": "SfUFsT51qUZG"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ],
   "metadata": {
    "id": "i7z2hlbrqn7e"
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/serializers.py:458\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcloudpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPickleError:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     70\u001B[0m cp \u001B[38;5;241m=\u001B[39m CloudPickler(\n\u001B[1;32m     71\u001B[0m     file, protocol\u001B[38;5;241m=\u001B[39mprotocol, buffer_callback\u001B[38;5;241m=\u001B[39mbuffer_callback\n\u001B[1;32m     72\u001B[0m )\n\u001B[0;32m---> 73\u001B[0m \u001B[43mcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001B[0m, in \u001B[0;36mCloudPickler.dump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001B[0m, in \u001B[0;36mCloudPickler.reducer_override\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, types\u001B[38;5;241m.\u001B[39mFunctionType):\n\u001B[0;32m--> 692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_function_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;66;03m# fallback to save_global, including the Pickler's\u001B[39;00m\n\u001B[1;32m    695\u001B[0m     \u001B[38;5;66;03m# dispatch_table\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001B[0m, in \u001B[0;36mCloudPickler._function_reduce\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 565\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamic_function_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001B[0m, in \u001B[0;36mCloudPickler._dynamic_function_reduce\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    545\u001B[0m newargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_getnewargs(func)\n\u001B[0;32m--> 546\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43m_function_getstate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (types\u001B[38;5;241m.\u001B[39mFunctionType, newargs, state, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    548\u001B[0m         _function_setstate)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001B[0m, in \u001B[0;36m_function_getstate\u001B[0;34m(func)\u001B[0m\n\u001B[1;32m    146\u001B[0m slotstate \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__name__\u001B[39m\u001B[38;5;124m\"\u001B[39m: func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__qualname__\u001B[39m\u001B[38;5;124m\"\u001B[39m: func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__closure__\u001B[39m\u001B[38;5;124m\"\u001B[39m: func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__closure__\u001B[39m,\n\u001B[1;32m    155\u001B[0m }\n\u001B[0;32m--> 157\u001B[0m f_globals_ref \u001B[38;5;241m=\u001B[39m \u001B[43m_extract_code_globals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__code__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m f_globals \u001B[38;5;241m=\u001B[39m {k: func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__globals__\u001B[39m[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m f_globals_ref \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m    159\u001B[0m              func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__globals__\u001B[39m}\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001B[0m, in \u001B[0;36m_extract_code_globals\u001B[0;34m(co)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# We use a dict with None values instead of a set to get a\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;66;03m# non-deterministic pickle bytes as a results.\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m out_names \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m[\u001B[49m\u001B[43moparg\u001B[49m\u001B[43m]\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moparg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_walk_global_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43mco\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001B[39;00m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001B[39;00m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;66;03m# of the nested function's As the nested function may itself need\u001B[39;00m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;66;03m# global variables, we need to introspect its code, extract its\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001B[39;00m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;66;03m# add the result to code_globals\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# We use a dict with None values instead of a set to get a\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;66;03m# non-deterministic pickle bytes as a results.\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m out_names \u001B[38;5;241m=\u001B[39m {\u001B[43mnames\u001B[49m\u001B[43m[\u001B[49m\u001B[43moparg\u001B[49m\u001B[43m]\u001B[49m: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _, oparg \u001B[38;5;129;01min\u001B[39;00m _walk_global_ops(co)}\n\u001B[1;32m    336\u001B[0m \u001B[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001B[39;00m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001B[39;00m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;66;03m# of the nested function's As the nested function may itself need\u001B[39;00m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;66;03m# global variables, we need to introspect its code, extract its\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001B[39;00m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;66;03m# add the result to code_globals\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/session.py:891\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    888\u001B[0m     has_pandas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pandas\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m--> 891\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mSparkSession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[1;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m    895\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    896\u001B[0m )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:437\u001B[0m, in \u001B[0;36mSparkConversionMixin.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    435\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    436\u001B[0m converted_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_from_pandas(data, schema, timezone)\n\u001B[0;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconverted_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/session.py:938\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    936\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m    937\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 938\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(\u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_java_object_rdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    939\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\u001B[1;32m    940\u001B[0m df \u001B[38;5;241m=\u001B[39m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/rdd.py:3113\u001B[0m, in \u001B[0;36mRDD._to_java_object_rdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   3110\u001B[0m rdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pickled()\n\u001B[1;32m   3111\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mpythonToJava(\u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001B[0m, in \u001B[0;36mPipelinedRDD._jrdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   3502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3503\u001B[0m     profiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3505\u001B[0m wrapped_func \u001B[38;5;241m=\u001B[39m \u001B[43m_wrap_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prev_jrdd_deserializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd_deserializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofiler\u001B[49m\n\u001B[1;32m   3507\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3509\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3510\u001B[0m python_rdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonRDD(\n\u001B[1;32m   3511\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prev_jrdd\u001B[38;5;241m.\u001B[39mrdd(), wrapped_func, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreservesPartitioning, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_barrier\n\u001B[1;32m   3512\u001B[0m )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001B[0m, in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   3360\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m serializer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserializer should not be empty\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3361\u001B[0m command \u001B[38;5;241m=\u001B[39m (func, profiler, deserializer, serializer)\n\u001B[0;32m-> 3362\u001B[0m pickled_command, broadcast_vars, env, includes \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_for_python_RDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3363\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3364\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonFunction(\n\u001B[1;32m   3365\u001B[0m     \u001B[38;5;28mbytearray\u001B[39m(pickled_command),\n\u001B[1;32m   3366\u001B[0m     env,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3371\u001B[0m     sc\u001B[38;5;241m.\u001B[39m_javaAccumulator,\n\u001B[1;32m   3372\u001B[0m )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001B[0m, in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   3342\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prepare_for_python_RDD\u001B[39m(sc: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkContext\u001B[39m\u001B[38;5;124m\"\u001B[39m, command: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mbytes\u001B[39m, Any, Any, Any]:\n\u001B[1;32m   3343\u001B[0m     \u001B[38;5;66;03m# the serialized command will be compressed by broadcast\u001B[39;00m\n\u001B[1;32m   3344\u001B[0m     ser \u001B[38;5;241m=\u001B[39m CloudPickleSerializer()\n\u001B[0;32m-> 3345\u001B[0m     pickled_command \u001B[38;5;241m=\u001B[39m \u001B[43mser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3346\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3347\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pickled_command) \u001B[38;5;241m>\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mgetBroadcastThreshold(sc\u001B[38;5;241m.\u001B[39m_jsc):  \u001B[38;5;66;03m# Default 1M\u001B[39;00m\n\u001B[1;32m   3348\u001B[0m         \u001B[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/serializers.py:468\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    466\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not serialize object: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (e\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, emsg)\n\u001B[1;32m    467\u001B[0m print_exec(sys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[0;32m--> 468\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPicklingError(msg)\n",
      "\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syetJUMdrBci",
    "outputId": "d05efb8f-3a72-4213-85fe-19673f282e8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- title_cleaned: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- kurzfassung_cleaned: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# word vectors for text columns\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "def word_vectorizer(df, col, col_new):\n",
    "\n",
    "    #create an average word vector for text columns\n",
    "    word2vec = Word2Vec(vectorSize = 10, minCount = 3, inputCol = col, outputCol = col_new)\n",
    "    model = word2vec.fit(df)\n",
    "    message_w2vec = model.transform(df)\n",
    "\n",
    "    # drop original column\n",
    "    message_w2vec = message_w2vec.drop(col)\n",
    "\n",
    "    return message_w2vec\n",
    "\n",
    "# Apply function for text columns\n",
    "# titel\n",
    "col = 'title_cleaned'\n",
    "col_new = 'title_vectorized'\n",
    "df6 =  word_vectorizer(spark_df, col, col_new)\n",
    "\n",
    "# kurzfassung\n",
    "col = 'kurzfassung_cleaned'\n",
    "col_new = 'kurzfassung_vectorized'\n",
    "df7 =  word_vectorizer(df6, col, col_new)"
   ],
   "metadata": {
    "id": "gSg4IbFDqm8S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df7.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJm_ws6Nr3wq",
    "outputId": "4be805e2-dbc9-4c65-d23e-03007614c252"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- title_vectorized: vector (nullable = true)\n",
      " |-- kurzfassung_vectorized: vector (nullable = true)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n"
   ],
   "metadata": {
    "id": "hJmYEXGjrfWh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# string indexer for author\n",
    "\n",
    "def category_index(df, col, col_new):\n",
    "\n",
    "    indexer = StringIndexer(inputCol = col, outputCol = col_new)\n",
    "    model = indexer.fit(df)\n",
    "    index_df = model.transform(df)\n",
    "    \n",
    "    #drop old columns\n",
    "    index_df = index_df.drop(col)\n",
    "\n",
    "    return index_df\n",
    "\n",
    "#apply function for author\n",
    "\n",
    "col = 'author'\n",
    "col_new = 'author_index'\n",
    "df8 = category_index(df7, col, col_new)\n"
   ],
   "metadata": {
    "id": "V-yEG3BerpzA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df8.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBBV_qNGt3C9",
    "outputId": "13c871c6-97a1-4123-f16f-786f9a78781d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- title_vectorized: vector (nullable = true)\n",
      " |-- kurzfassung_vectorized: vector (nullable = true)\n",
      " |-- author_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# one hot encoder\n",
    "\n",
    "def encoding(df, col, col_new):\n",
    "\n",
    "        onehot_encoder = OneHotEncoder(inputCol = col, outputCol = col_new)\n",
    "        onehot_df = onehot_encoder.fit(df).transform(df)\n",
    "        \n",
    "        #drop old column\n",
    "        onehot_df = onehot_df.drop(col)\n",
    "        \n",
    "        return onehot_df\n",
    "\n",
    "    \n",
    "\n",
    "#apply function\n",
    "\n",
    "\n",
    "col = 'author_index'\n",
    "col_new = 'author_encoded'\n",
    "df9 = encoding(df8, col, col_new)\n"
   ],
   "metadata": {
    "id": "2dhs6MHjsWkF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df9.printSchema()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYdCWuMtt6VQ",
    "outputId": "25ace439-29a9-4823-bc11-e106e2ee7a18"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- sdg: long (nullable = true)\n",
      " |-- title_vectorized: vector (nullable = true)\n",
      " |-- kurzfassung_vectorized: vector (nullable = true)\n",
      " |-- author_encoded: vector (nullable = true)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df9_grouped = df9.groupby('sdg').count()\n",
    "df9_grouped.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkJvic94vf3S",
    "outputId": "4e123c68-6faf-45c1-bd5f-c0a0755f48aa"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+\n",
      "|sdg|count|\n",
      "+---+-----+\n",
      "|  7|   17|\n",
      "|  6|    3|\n",
      "|  9|   14|\n",
      "|  5|   23|\n",
      "|  1|    6|\n",
      "| 10|   33|\n",
      "|  3|   44|\n",
      "| 12|   31|\n",
      "|  8|   26|\n",
      "| 11|    3|\n",
      "|  2|   14|\n",
      "|  4|    2|\n",
      "| 17|    8|\n",
      "| 13|   41|\n",
      "| 14|   14|\n",
      "| 15|   88|\n",
      "| 16|   34|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Model\n",
    "In this section the main model is trained."
   ],
   "metadata": {
    "id": "ueidnpycZ3QL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# data split into train and test\n",
    "\n",
    "(trainDF, testDF) = df9.randomSplit([0.7, 0.3], seed=12)\n",
    "\n",
    "trainDF2 = trainDF.groupby('sdg').count()\n",
    "trainDF2.show()\n",
    "\n",
    "testDF2 = testDF.groupby('sdg').count()\n",
    "testDF2.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49imssGvzKTv",
    "outputId": "496e9664-8335-4fb8-a22d-8c53ca6c742e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+\n",
      "|sdg|count|\n",
      "+---+-----+\n",
      "|  7|   13|\n",
      "|  6|    1|\n",
      "|  9|    9|\n",
      "|  5|   16|\n",
      "|  1|    4|\n",
      "| 10|   25|\n",
      "|  3|   33|\n",
      "| 12|   24|\n",
      "|  8|   17|\n",
      "| 11|    2|\n",
      "|  2|   11|\n",
      "|  4|    1|\n",
      "| 17|    5|\n",
      "| 13|   30|\n",
      "| 14|   11|\n",
      "| 15|   66|\n",
      "| 16|   25|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "|sdg|count|\n",
      "+---+-----+\n",
      "|  7|    4|\n",
      "|  6|    2|\n",
      "|  9|    5|\n",
      "|  5|    7|\n",
      "|  1|    2|\n",
      "| 10|    8|\n",
      "|  3|   11|\n",
      "| 12|    7|\n",
      "|  8|    9|\n",
      "| 11|    1|\n",
      "|  2|    3|\n",
      "|  4|    1|\n",
      "| 17|    3|\n",
      "| 13|   11|\n",
      "| 14|    3|\n",
      "| 15|   22|\n",
      "| 16|    9|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# main model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MulticlassLogisticRegressionWithElasticNet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df10 = df9.withColumnRenamed('sdg', 'label')\n",
    "\n",
    "feature = df10.drop(\"label\").columns\n",
    "print(feature)\n",
    "\n",
    "\n",
    "\n",
    "(trainDF, testDF) = df10.randomSplit([0.7, 0.3], seed=12)\n",
    "\n",
    "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "vecAssembler = VectorAssembler(inputCols=feature, outputCol=\"features\")\n",
    "\n",
    "# maxIter=10, regParam=0.3, elasticNetParam=0.8\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#Build the Pipeline\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "\n",
    "# Create model.\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "    \n",
    "print(\"#######\\n#######\\n#######\\n\")\n",
    "print(\"Now printing: Model test\")\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i+1, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i+1, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "    % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n",
    "\n",
    "print(\"#######\\n#######\\n#######\\n\")\n",
    "print(\"Now printing: Model validation prediction\")\n",
    "\n",
    "# Apply the pipeline model to the test dataset.\n",
    "validation_prediction = pipelineModel.transform(testDF)\n",
    "\n",
    "#Display the predictions from the model. The features column is a sparse vector, which is often the case after one-hot encoding, #because there are so many 0 values\n",
    "validation_prediction.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "#Evaluate the model\n",
    "#The display command has a built-in ROC curve option.\n",
    "\n",
    "print(pipelineModel.stages[-1], validation_prediction.drop(\"prediction\", \"rawPrediction\"), \"ROC\")\n",
    "\n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(validation_prediction)}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LOx7O9-t_nW",
    "outputId": "e691c690-27f9-45a8-fef5-4be4cd14de6e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['title_vectorized', 'kurzfassung_vectorized', 'author_encoded']\n",
      "#######\n",
      "#######\n",
      "#######\n",
      "\n",
      "Now printing: Model test\n",
      "Coefficients: \n",
      "DenseMatrix([[-5.03405055e+00, -1.38865201e+01, -2.35882543e+01, ...,\n",
      "               1.19446393e+00,  1.02759590e+00,  0.00000000e+00],\n",
      "             [ 3.56193682e+03,  2.88795751e+03,  2.19657854e+02, ...,\n",
      "              -6.38054788e+00, -7.18727113e+00,  0.00000000e+00],\n",
      "             [ 4.12383394e+03, -1.42694535e+02, -2.22173403e+03, ...,\n",
      "              -1.94157370e+01, -1.46260767e+01,  0.00000000e+00],\n",
      "             ...,\n",
      "             [ 1.30094275e+03, -1.44514893e+03,  3.30818395e+02, ...,\n",
      "              -5.12907274e+01, -7.92185221e+01,  0.00000000e+00],\n",
      "             [-1.23788876e+03,  8.26044002e+02,  9.68415385e+02, ...,\n",
      "              -3.35969158e+01, -3.58283921e+01,  0.00000000e+00],\n",
      "             [-3.45296280e+03, -2.89818853e+03,  8.90036255e+02, ...,\n",
      "              -2.12246066e+00, -6.22629438e+00,  0.00000000e+00]])\n",
      "Intercept: [-1.1742065082124367,251.45426669670712,-94.66520238793541,-30.608642202936192,-33.335786345350684,-212.95773078315827,-145.09875695561215,69.98399549922333,-130.4993949844348,-22.034591989277246,-134.11517595994889,25.114474823841714,93.14304720788752,78.36344313553832,-29.88110808972242,151.52111712734833,61.56421088559238,103.22604083044976]\n",
      "objectiveHistory:\n",
      "2.467707240985234\n",
      "1.6855219269360429\n",
      "0.7186420415662563\n",
      "0.4615964756290286\n",
      "0.3380181565377038\n",
      "0.25781953706238514\n",
      "0.1971214883326888\n",
      "0.14291056071105648\n",
      "0.10806779669059739\n",
      "0.0890981085536323\n",
      "0.07447398442367166\n",
      "0.06417199976361304\n",
      "0.05293190965258524\n",
      "0.043742252029943814\n",
      "0.034717203773560426\n",
      "0.02618988594335607\n",
      "0.02012228499123432\n",
      "0.016119938833244647\n",
      "0.005146269620837748\n",
      "0.003173792806298687\n",
      "0.0007358764021678975\n",
      "0.0005648607409329225\n",
      "0.00026530850913347156\n",
      "0.0001446200850549123\n",
      "7.144374600237351e-05\n",
      "3.6798877206700225e-05\n",
      "1.8694071668123924e-05\n",
      "9.525848423625256e-06\n",
      "4.786682080604029e-06\n",
      "2.4063140156690254e-06\n",
      "1.2116884825345035e-06\n",
      "5.268310725153452e-07\n",
      "2.2085181302913678e-07\n",
      "1.412130477389402e-07\n",
      "6.590115830621232e-08\n",
      "3.457518247454367e-08\n",
      "1.5515317800472005e-08\n",
      "9.820170230927707e-09\n",
      "4.811844225500038e-09\n",
      "2.546488332220875e-09\n",
      "False positive rate by label:\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "label 3: 0.0\n",
      "label 4: 0.0\n",
      "label 5: 0.0\n",
      "label 6: 0.0\n",
      "label 7: 0.0\n",
      "label 8: 0.0\n",
      "label 9: 0.0\n",
      "label 10: 0.0\n",
      "label 11: 0.0\n",
      "label 12: 0.0\n",
      "label 13: 0.0\n",
      "label 14: 0.0\n",
      "label 15: 0.0\n",
      "label 16: 0.0\n",
      "label 17: 0.0\n",
      "True positive rate by label:\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "label 3: 1.0\n",
      "label 4: 1.0\n",
      "label 5: 1.0\n",
      "label 6: 1.0\n",
      "label 7: 1.0\n",
      "label 8: 1.0\n",
      "label 9: 1.0\n",
      "label 10: 1.0\n",
      "label 11: 1.0\n",
      "label 12: 1.0\n",
      "label 13: 1.0\n",
      "label 14: 1.0\n",
      "label 15: 1.0\n",
      "label 16: 1.0\n",
      "label 17: 1.0\n",
      "Precision by label:\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "label 3: 1.0\n",
      "label 4: 1.0\n",
      "label 5: 1.0\n",
      "label 6: 1.0\n",
      "label 7: 1.0\n",
      "label 8: 1.0\n",
      "label 9: 1.0\n",
      "label 10: 1.0\n",
      "label 11: 1.0\n",
      "label 12: 1.0\n",
      "label 13: 1.0\n",
      "label 14: 1.0\n",
      "label 15: 1.0\n",
      "label 16: 1.0\n",
      "label 17: 1.0\n",
      "Recall by label:\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "label 3: 1.0\n",
      "label 4: 1.0\n",
      "label 5: 1.0\n",
      "label 6: 1.0\n",
      "label 7: 1.0\n",
      "label 8: 1.0\n",
      "label 9: 1.0\n",
      "label 10: 1.0\n",
      "label 11: 1.0\n",
      "label 12: 1.0\n",
      "label 13: 1.0\n",
      "label 14: 1.0\n",
      "label 15: 1.0\n",
      "label 16: 1.0\n",
      "label 17: 1.0\n",
      "F-measure by label:\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "label 3: 1.0\n",
      "label 4: 1.0\n",
      "label 5: 1.0\n",
      "label 6: 1.0\n",
      "label 7: 1.0\n",
      "label 8: 1.0\n",
      "label 9: 1.0\n",
      "label 10: 1.0\n",
      "label 11: 1.0\n",
      "label 12: 1.0\n",
      "label 13: 1.0\n",
      "label 14: 1.0\n",
      "label 15: 1.0\n",
      "label 16: 1.0\n",
      "label 17: 1.0\n",
      "Accuracy: 1.0\n",
      "FPR: 0.0\n",
      "TPR: 1.0\n",
      "F-measure: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "#######\n",
      "#######\n",
      "#######\n",
      "\n",
      "Now printing: Model validation prediction\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(135,[0,1,2,3,4,5...|    1|      10.0|\n",
      "|(135,[0,1,2,3,4,5...|    1|       1.0|\n",
      "|(135,[0,1,2,3,4,5...|    2|       2.0|\n",
      "|(135,[0,1,2,3,4,5...|    2|       2.0|\n",
      "|(135,[0,1,2,3,4,5...|    2|       2.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       5.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|      16.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       3.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|      16.0|\n",
      "|(135,[0,1,2,3,4,5...|    3|       8.0|\n",
      "|(135,[0,1,2,3,4,5...|    4|      16.0|\n",
      "|(135,[0,1,2,3,4,5...|    5|       5.0|\n",
      "|(135,[0,1,2,3,4,5...|    5|       5.0|\n",
      "|(135,[0,1,2,3,4,5...|    5|       3.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "LogisticRegressionModel: uid=LogisticRegression_9243835933d1, numClasses=18, numFeatures=135 DataFrame[label: bigint, title_vectorized: vector, kurzfassung_vectorized: vector, author_encoded: vector, features: vector, probability: vector] ROC\n",
      "Accuracy: 0.5233644859813084\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Quality check on how the model performs on each different SDG\n",
    "from pyspark.sql.functions import sum, cols_to_drop_later, desc\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "validation_count = validation_prediction.groupby('label', 'prediction').count()\n",
    "validation_count = validation_count.sort(desc(\"count\"))\n",
    "validation_count.show(200)\n",
    "y = validation_prediction.count()\n",
    "\n",
    "\n",
    "data_collect = validation_count.collect()\n",
    "\n",
    "x = 0\n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_collect:\n",
    "    if row['label'] == row['prediction']:\n",
    "      x += row['count']\n",
    "\n",
    "acc = x / y\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "to_check_sdg = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "d_error_by_label = {}\n",
    "cols_to_drop_later = ['label', 'prediction']\n",
    "\n",
    "for iii in to_check_sdg:\n",
    "  new_df_validation = validation_count.filter(cols_to_drop_later(\"label\") == iii)\n",
    "  dfxx = new_df_validation.toPandas()\n",
    "  listxx = dfxx['count'].tolist()\n",
    "  #print(listxx)\n",
    "  count_total = 0.0\n",
    "  for x in listxx:\n",
    "    count_total += x\n",
    "  count_total *= 1.0\n",
    "  print(\"count_total\",count_total)\n",
    "\n",
    "  dfxxx = dfxx[dfxx['label'] != dfxx['prediction']]\n",
    "  listxx = dfxxx['count'].tolist()\n",
    "  count_total_errors = 0.0\n",
    "  for x in listxx:\n",
    "    count_total_errors += x\n",
    "  count_total_errors *= 1.0\n",
    "  print(\"count_total_errors\",count_total_errors)\n",
    "  d_error_by_label[iii] = 1 - count_total_errors/count_total\n",
    "\n",
    "print(\"errors:\", d_error_by_label)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuFBo_o0VVg7",
    "outputId": "fe24dd55-ba32-4795-a497-28825158c7ab"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|   15|      15.0|   13|\n",
      "|    3|       3.0|    7|\n",
      "|   13|      13.0|    7|\n",
      "|   16|      16.0|    6|\n",
      "|    5|       5.0|    4|\n",
      "|    8|      10.0|    4|\n",
      "|    9|      12.0|    3|\n",
      "|   12|      12.0|    3|\n",
      "|    2|       2.0|    3|\n",
      "|    7|       7.0|    3|\n",
      "|   15|      13.0|    3|\n",
      "|   15|      14.0|    3|\n",
      "|   12|      17.0|    2|\n",
      "|   10|      17.0|    2|\n",
      "|   16|       3.0|    2|\n",
      "|   15|       1.0|    2|\n",
      "|    9|       9.0|    2|\n",
      "|    8|       3.0|    2|\n",
      "|    8|       9.0|    2|\n",
      "|    5|       8.0|    1|\n",
      "|    3|      14.0|    1|\n",
      "|    3|      16.0|    1|\n",
      "|    5|      12.0|    1|\n",
      "|    6|      13.0|    1|\n",
      "|   10|      12.0|    1|\n",
      "|   10|      16.0|    1|\n",
      "|   10|       5.0|    1|\n",
      "|   11|      14.0|    1|\n",
      "|    8|       8.0|    1|\n",
      "|   10|      10.0|    1|\n",
      "|   13|      15.0|    1|\n",
      "|    3|      10.0|    1|\n",
      "|    1|       8.0|    1|\n",
      "|    4|       8.0|    1|\n",
      "|    5|      10.0|    1|\n",
      "|    7|       2.0|    1|\n",
      "|    1|       1.0|    1|\n",
      "|   14|      14.0|    1|\n",
      "|   15|       8.0|    1|\n",
      "|   16|      10.0|    1|\n",
      "|   14|       8.0|    1|\n",
      "|   17|      15.0|    1|\n",
      "|   13|      17.0|    1|\n",
      "|   12|      10.0|    1|\n",
      "|   10|       8.0|    1|\n",
      "|    6|      10.0|    1|\n",
      "|   10|       3.0|    1|\n",
      "|    3|       5.0|    1|\n",
      "|   17|      10.0|    1|\n",
      "|   17|      12.0|    1|\n",
      "|   13|      12.0|    1|\n",
      "|   14|      15.0|    1|\n",
      "|   13|      10.0|    1|\n",
      "|   12|       6.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n",
      "accuracy: 0.48148148148148145\n",
      "count_total 2.0\n",
      "count_total_errors 1.0\n",
      "count_total 3.0\n",
      "count_total_errors 0.0\n",
      "count_total 11.0\n",
      "count_total_errors 4.0\n",
      "count_total 1.0\n",
      "count_total_errors 1.0\n",
      "count_total 7.0\n",
      "count_total_errors 3.0\n",
      "count_total 2.0\n",
      "count_total_errors 2.0\n",
      "count_total 4.0\n",
      "count_total_errors 1.0\n",
      "count_total 9.0\n",
      "count_total_errors 8.0\n",
      "count_total 5.0\n",
      "count_total_errors 3.0\n",
      "count_total 8.0\n",
      "count_total_errors 7.0\n",
      "count_total 1.0\n",
      "count_total_errors 1.0\n",
      "count_total 7.0\n",
      "count_total_errors 4.0\n",
      "count_total 11.0\n",
      "count_total_errors 4.0\n",
      "count_total 3.0\n",
      "count_total_errors 2.0\n",
      "count_total 22.0\n",
      "count_total_errors 9.0\n",
      "count_total 9.0\n",
      "count_total_errors 3.0\n",
      "count_total 3.0\n",
      "count_total_errors 3.0\n",
      "errors: {1: 0.5, 2: 1.0, 3: 0.6363636363636364, 4: 0.0, 5: 0.5714285714285714, 6: 0.0, 7: 0.75, 8: 0.11111111111111116, 9: 0.4, 10: 0.125, 11: 0.0, 12: 0.4285714285714286, 13: 0.6363636363636364, 14: 0.33333333333333337, 15: 0.5909090909090908, 16: 0.6666666666666667, 17: 0.0}\n"
     ]
    }
   ]
  }
 ]
}
